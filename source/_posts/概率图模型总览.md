---
layout: post
title: 概率图模型总览
toc_number_disable: true
draft: false
comments: false
date: 2016-05-15 15:03:17
categories:
  - "概率图模型"
tags:
  - "有向无环图"
  - "无向图"
  - "概率推断"
  - "概率参数估计"
cover_img: /blog/post_cover_images/bayes_avator.png
cover_img_from_root: true
permalink:
description:
---

## 1. 引言

机器学习中最重要的任务，可以看做根据已观测的数据证据（如训练样本）来对感兴趣的未知变量（如分类标签）进行估计和推测。概率图模型（probabilistic graphical model, PGM），是一种学习任务的框架描述，它将学习任务归结为计算变量的概率分布[^1]。

> 推断（inference）：在概率模型中，利用已知变量推测未知变量的分布的过程即为推断。在学习任务中，不仅仅是预测问题可以看做是一个推断过程，典型的“有因溯果”的逆向推导也可以看做是推断过程。

从推断的角度出发，给定一个问题中关心的变量集合$Y$，可观测变量集合$O$，其他变量集合为$R$，两种不同学习模型的策略可以概括如下：
* 生成（generative）模型：其考虑的是联合概率分布$P(Y,O,R)$，并由此推断条件概率$P(O \mid Y)$
* 判别（discriminative）模型：其考虑的是条件概率分布$P(Y,R \mid O)$，并由此推断条件概率$P(O \mid Y)$

想要直接通过概率求和规则的方式来消去变量$R$并得到条件概率$P(O \mid Y)$，复杂度高达$O(n^{|Y|+|R|})$，假定每个变量取值有$n$种。同时，各个随机变量的复杂联系也使得通过训练样本来获知变量分布的参数往往十分困难。在这种情况下，概率图模型，提出了使用图的方式来表达变量相关概率关系，以用于实现高效的推断和学习算法。

## 2. 体系

### 2.1 相关理论分类

1. PGM表示理论
  * 研究如何利用概率网络中的独立性来简化联合概率分布的方法表示。
  * 概率图模型的表示分为参数和结构两部分，需要分别进行确定。
2. PGM学习理论
  * 概率图模型学习算法分为参数学习与结构学习。
  * 参数学习算法根据数据集是否完备而分为确定性不完备和随机性不完备下的学习算法
  * 针对结构学习算法特点的不同，结构学习算法归纳为基于约束的学习、基于评分搜索的学习、混合学习、动态规划结构学习、模型平均结构学习和不完备数据集的结构学习。
3. PGM推断理论
  * 贝叶斯网络与马尔可夫网络中解决概率查询问题的*精确推理算法*与*近似推理算法*
  * 确切推断(exact inference)的复杂度取决于模型的tree width。对于很多实际模型，这个复杂度可能随着问题规模增长而指数增长。
  * 人们退而求其次，转而探索具有多项式复杂度的近似推断(approximate inference)方法：
    * 基于平均场逼近（mean field approximation）的variational inference：EM算法就属于这类型算法的一种特例。
    * 信念传播（belief propagation）：将变量消去法中的求和操作看做一个消息传递过程，消息传递相关的计算被限制在图的局部进行。
    * 蒙特卡罗采样（Monte Carlo sampling）：蒙特卡罗方法通过对概率模型的随机模拟运行来收集样本，然后通过收集到的样本来估计变量的统计特性（比如，均值）。

### 2.2 模型分类

按照概率图中变量关系的不同，概率图模型可以大致分为两类：
* 贝叶斯网络：有向图模型，使用有向无环图表达关系（通常，变量间存在显式的因果关系）
* 马尔科夫网络：无向图模型，使用无图表达关系（通常，变量间存有关系，但是难以显式表达）

> 有时候，也将同时存有有向边和无向边的模型，如条件随机场（conditional random field）和链图（chain graph），单独看做一类局部有向模型。

贝叶斯网络可以分为静态贝叶斯网络和动态贝叶斯网络。相比于[静态贝叶斯网络](/blog/静态贝叶斯网络)，动态（dynamic）贝叶斯网络主要用于时序数据建模（如语音识别、自然语言处理、轨迹数据挖掘等）。其中，一种结构最简单的动态贝叶斯网络就是隐马尔可夫模型（hidden markov model, HMM）。

一般来说，贝叶斯网络中每一个结点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积。而对于马尔可夫网络，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。

按照表示的抽象级别不同，概率图模型可以分为：
* 基于随机变量的概率图模型，如贝叶斯网、马尔可夫网、条件随机场和链图等；
* 基于**模板**的概率图模型．这类模型根据应用场景不同又可分为两种：
  * 暂态模型，包括动态贝叶斯网（Dynamic Bayesian Network, DBN）和状态观测模型，其中状态观测模型又包括线性动态系统（Linear Dynamic System, LDS）和隐马尔可夫模型（Hidden Markov Model, HMM）；
  * 对象关系领域的概率图模型，包括盘模型（Plate Model，PM）、概率关系模型（Probabilistic Relational Model, PRM）和关系马尔可夫网（Relational Markov Network, RMN）。

## 3. 概率图模型的通用解释

> 以下引用自[^3]

机器学习的一个核心任务是从观测到的数据中挖掘隐含的知识，而概率图模型是实现这一任务的一种优雅手段。其巧妙地结合了图论和概率论：
  * 从图论的角度，PGM是一个图，包含结点与边。结点可以分为两类：隐含结点和观测结点。边可以是有向的或者是无向的。
  * 从概率论的角度，PGM是一个概率分布，图中的结点对应于随机变量，边对应于随机变量的dependency或者correlation关系。
给定一个实际问题，我们通常会观测到一些数据，并且希望能够挖掘出隐含在数据中的知识。怎么用PGM实现呢？我们构建一个图，用观测结点表示观测到的数据，用隐含结点表示潜在的知识，用边来描述知识与数据的相互关系，最后获得一个概率分布。给定概率分布之后，通过进行两个任务：inference（给定观测结点，推断隐含结点的后验分布）和learning（学习这个概率分布的参数），来获取知识。PGM的强大之处在于，不管数据和知识多复杂，我们的处理手段是一样的：建一个图，定义一个概率分布，进行推断和学习。

## 4. 概率图模型的参数估计和推理算法

### 4.1 参数估计

* 对于贝叶斯网络中某个结点的条件概率分布（即条件概率表），可以通过计算训练数据中时间发生的次数来统计，这样获得的参数能够最大化被观测到的数据的可能性。
* 对于马尔科夫网络，上述计数方法没有统计学上的支持因而会得到次优的参数。一般而言，对于马尔科夫网络的参数估计，其指导思想为梯度下降，即定义一些描述分布的参数，然后使用梯度下降来寻找能最大化被观测数据可能性的参数值。

### 4.2 推断

当模型参数被确定后，我们需要在新的数据上进行使用，也就是进行相关的概率推断。使用推断，可以求解一些非常重要的问题
* 边缘推断（marginal inference）：寻找某个特定变量（结点）在不同取值上的概率分布。
* 后验推断（posterior inference）：给定某些显变量$v_E$，其观测取值为$e$，求某些隐藏变量$v_H$的后验分布$P(v_H \mid v_E = e)$。
* 最大后验推断（maximum-a-posterior inference）：给定某些显变量$v_E$，其观测取值为$e$，求某些隐藏变量$v_H$具有最高概率的参数配置。

> 边缘分布是指对无关变量求和或积分后得到的结果。例如，在马尔科夫网中，变量的联合分布被表示为极大团的势函数乘积，于是，给定参数$\Theta$求解某个变量$x$的分布，就变成联合分布中其他无关变量进行积分的过程，这称之为“边缘化”。

无论是上述哪一种推断，我们可以按照引用[^1]中的形式进行较为一般的形式化定义：

假设图模型对应的变量集$\mathbf{x} = \{ x_1, \ldots, x_N \}$能够分为$\mathbf{x}_E$和$\mathbf{x}_F$两个不相交的变量集。推断问题的目标就是计算边缘概率$P(\mathbf{x}_F)$或者条件概率$P(\mathbf{x}_F \mid \mathbf{x}_E)$。由条件概率有：

> $P(\mathbf{x}_F \mid \mathbf{x}_E) = \frac{P(\mathbf{x}_E, \mathbf{x}_F)}{P(\mathbf{x}_E)} = \frac{P(\mathbf{x}_E, \mathbf{x}_F)}{\sum_{\mathbf{x}_F}P(\mathbf{x}_E, \mathbf{x}_F)}$

由于联合概率$P(\mathbf{x}_E, \mathbf{x}_F)$可以基于概率图模型获得，因此，腿短问题的关键在于如何高效地计算边缘分布，即

> $P(\mathbf{x}_E) = \sum_{\mathbf{x}_F}P(\mathbf{x}_E, \mathbf{x}_F)$

解答这些问题，既有精确推断算法，也有近似推断算法（推断在计算上很困难！在某些特定类型的图中我们可以相当高效地执行推断，但一般而言图的计算都很难。所以我们需要使用近似算法来在准确度和效率之间进行权衡）。
* 精确推断算法：希望能够计算出目标变量的边缘分布或条件分布的精确值；一般情况下，算法的计算复杂度随最大团规模的增长而指数增长，适用范围有限；变量消除和信念传播是两种较为常见的精确推断算法。
* 近似推断算法：在较低的时间复杂度下获得原问题的近似解；一般较为常用；采样和变分推断是两种常见的近似推断算法。

#### 4.2.1 变量消除（Variable Elimination）

#### 4.2.2 信念传播（Belief Propagation）

#### 4.2.3 基于采样的近视推断

#### 4.2.4 变分推断

## 学习资料

* Judea Pearl: Networks of Plausible Inference.
* Daphne Koller: Probabilistic Graphical Models: Principles and Techniques.
* [CMU-course 10708 Probabilistic Graphical Models](http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html).
* [Coursera | Probabilistic Graphical Models](https://www.coursera.org/specializations/probabilistic-graphical-models).
* [Charles Sutton and Andrew McCallum: An Introduction to Conditional Random Fields](https://arxiv.org/abs/1011.4088).


## 引用

[^1]: 《机器学习》，周志华著，清华大学出版社.
[^2]: [概率图模型，百度百科](https://baike.baidu.com/item/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/2120179?fr=aladdin#reference-[2]-8541105-wrap).
[^3]: [https://www.zhihu.com/question/23255632/answer/56330768](https://www.zhihu.com/question/23255632/answer/56330768)
[^4]: Graphical Models in a Nutshell.
[^5]: [读懂概率图模型，Prasoon Goyal](http://www.sohu.com/a/207319466_465975).
